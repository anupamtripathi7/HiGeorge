{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Part 1: Classifying Crimes with Machine Learning</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ''\n",
    "data_path = os.path.join(root, 'data')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>For the given task, we look to build a pipeline for which we have the following subtasks:<br>\n",
    "1) Data Exploration<br>\n",
    "2) Data Preprocessing<br>\n",
    "3) Modeling\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>Property Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMV</td>\n",
       "      <td>Property Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>Property Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Petit Larceny</td>\n",
       "      <td>Property Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUTO THEFT/PASSENGER VEHICLE</td>\n",
       "      <td>Property Crime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    description        category\n",
       "0                 LARCENY/THEFT  Property Crime\n",
       "1                           BMV  Property Crime\n",
       "2                 LARCENY/THEFT  Property Crime\n",
       "3                 Petit Larceny  Property Crime\n",
       "4  AUTO THEFT/PASSENGER VEHICLE  Property Crime"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read train dataset\n",
    "train_df = pd.read_csv(os.path.join(data_path, 'Crime Descriptions - crime-descriptions.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_path, 'Crime Descriptions - uncategorized.csv'))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h3>1) Data Exploration: Analyze dataset for finding the best preprocessing and modeling techniques</h3>\n",
    "By looking at the data, we see that the preview descriptions are composed of key words and really short descriptions only, so language modeling might not be required, but lets see if thats true for the entire dataset. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['sentence_lenght'] = train_df['description'].apply(lambda x: len(x.split(' ')))\n",
    "_ = plt.hist(train_df['sentence_lenght'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here we can see that although most of the descriptions are really short, they can go upto lenght of 33. Therefore, language modeling might give better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(train_df['category'].astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are imbalanced, so if the class wise precision scores are not good enough, we will have to deal with it seperatly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h3>2) Data Preprocessing</h3>\n",
    "We will look to transform the string data to a machine readable format and also look to do some feature extraction which will help the model during the classification task \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the problem description, we know the 8 crime categories we need the classify the given text into. We also have a few examples for each categories, using which we can come up with an important key word dictionary which we will be used for keyword matching with the text description and pass as additional features to our model. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_dict = {'Property Crime': ['arson', 'burglar', 'break', 'enter', 'destruct', 'theft', 'car', 'vehicle', 'motor', 'steal', 'property', 'curfew', 'loitering', 'trespass'],\n",
    "               'Violent Crime': ['assault', 'homicide', 'rob', 'murder'],\n",
    "               'Substance-related Crime': ['drug', 'narcotic', 'influence', 'drunk', 'liquor', 'substance'],\n",
    "               'Sex Crime': ['pornography', 'obscene', 'prostitute', 'sex', 'peep', 'human-trafficking'],\n",
    "               'Personal Crime (nonviolent)': ['kidnap', 'abduct', 'runaway'],\n",
    "                'Financial Crime': ['bribery', 'counterfeiting', 'forgery', 'embezzlement', 'extrotion', 'blackmail', 'fraud'],\n",
    "               'No Crime Committed': [],\n",
    "               'Other': ['gambling', 'weapon', 'trafic']}\n",
    "encoding_dict = {'Property Crime': 0,\n",
    "               'Violent Crime': 1,\n",
    "               'Substance-related Crime': 2,\n",
    "               'Sex Crime': 3,\n",
    "               'Personal Crime (nonviolent)': 4,\n",
    "                'Financial Crime': 5,\n",
    "               'No Crime Committed': 6,\n",
    "               'Other': 7,\n",
    "                'Not Classified': -1}\n",
    "inc_encoding_dict = {}\n",
    "for d in encoding_dict:\n",
    "    inc_encoding_dict[encoding_dict[d]] = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though all keywords are in thier base form, we can pass all of them through the lematizer to make sure since all key word matching will take place in base form. \n",
    "For example, 'rock' and 'rocks' are the same word, but will not match on comparison and so we need compare both of them in base form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "inv_keyword_dict = {}\n",
    "\n",
    "for category in keyword_dict:\n",
    "    for n, keyword in enumerate(keyword_dict[category]):\n",
    "        keyword_dict[category][n] = nlp(keyword)[0].lemma_\n",
    "        inv_keyword_dict[keyword] = category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Property Crime', 'Violent Crime', 'Other',\n",
       "       'Substance-related Crime', 'Sex Crime', 'No Crime Committed',\n",
       "       'Financial Crime', 'Personal Crime (nonviolent)'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 567540/567540 [36:22<00:00, 260.07it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>sentence_lenght</th>\n",
       "      <th>word_vec</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMV</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Petit Larceny</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[4, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUTO THEFT/PASSENGER VEHICLE</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[5, 2, 1, 6, 7]</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567535</th>\n",
       "      <td>Theft, From Unlocked Vehicle, $200-$950</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[2, 37, 21, 333, 7, 37, 61, 280]</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567536</th>\n",
       "      <td>Petit Larceny</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[4, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567537</th>\n",
       "      <td>UUV</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[50]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567538</th>\n",
       "      <td>LARCENY/THEFT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567539</th>\n",
       "      <td>Burglary, Non-residential, Forcible Entry</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[23, 37, 248, 13, 666, 37, 120, 31]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567540 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      description category  sentence_lenght  \\\n",
       "0                                   LARCENY/THEFT        0                1   \n",
       "1                                             BMV        0                1   \n",
       "2                                   LARCENY/THEFT        0                1   \n",
       "3                                   Petit Larceny        0                2   \n",
       "4                    AUTO THEFT/PASSENGER VEHICLE        0                3   \n",
       "...                                           ...      ...              ...   \n",
       "567535    Theft, From Unlocked Vehicle, $200-$950        0                5   \n",
       "567536                              Petit Larceny        0                2   \n",
       "567537                                        UUV        0                1   \n",
       "567538                              LARCENY/THEFT        0                1   \n",
       "567539  Burglary, Non-residential, Forcible Entry        0                4   \n",
       "\n",
       "                                   word_vec                  features  \n",
       "0                                 [0, 1, 2]  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1                                       [3]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2                                 [0, 1, 2]  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3                                    [4, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4                           [5, 2, 1, 6, 7]  [2, 0, 0, 0, 0, 0, 0, 0]  \n",
       "...                                     ...                       ...  \n",
       "567535     [2, 37, 21, 333, 7, 37, 61, 280]  [2, 0, 0, 0, 0, 0, 0, 0]  \n",
       "567536                               [4, 0]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "567537                                 [50]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "567538                            [0, 1, 2]  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "567539  [23, 37, 248, 13, 666, 37, 120, 31]  [0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "\n",
       "[567540 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['word_vec'] = train_df['description'].astype(object)\n",
    "train_df['features'] = train_df['description'].astype(object)\n",
    "word_dict = {}\n",
    "word_dict_inv = {}\n",
    "i = 0\n",
    "for idx, row in tqdm(train_df.iterrows(), total=train_df.shape[0]):\n",
    "    sentence = row['description']\n",
    "    keywords_matched = {'Property Crime': 0,\n",
    "                       'Violent Crime': 0,\n",
    "                       'Substance-related Crime': 0,\n",
    "                       'Sex Crime': 0,\n",
    "                       'Personal Crime (nonviolent)': 0,\n",
    "                        'Financial Crime': 0,\n",
    "                       'No Crime Committed': 0,\n",
    "                       'Other': 0}\n",
    "    sentence = nlp(sentence)\n",
    "    new_sentence = []\n",
    "    for n, word in enumerate(sentence):\n",
    "        word_lower = word.lemma_.lower()\n",
    "        if word_lower not in word_dict:\n",
    "            word_dict[word_lower] = i\n",
    "            word_dict_inv[i] = word_lower\n",
    "            i += 1\n",
    "        new_sentence.append(word_dict[word_lower])\n",
    "        if word_lower in inv_keyword_dict:\n",
    "            keywords_matched[inv_keyword_dict[word_lower]] += 1\n",
    "    train_df.at[idx, 'word_vec'] = new_sentence\n",
    "    train_df.at[idx, 'features'] = list(keywords_matched.values())\n",
    "    train_df.at[idx, 'category'] = encoding_dict[train_df.at[idx, 'category']]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our vocab from our train set. We need to use the same vocab to create word vec of our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict['<UNK>'] = i\n",
    "word_dict['<PAD>'] = i+1\n",
    "word_dict_inv[i] = '<UNK>'\n",
    "word_dict_inv[i+1] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [27:58<00:00, 297.81it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>word_vec</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FRAUD - ALL OTHER</td>\n",
       "      <td>[110, 13, 186, 35]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6G</td>\n",
       "      <td>[1661, 145]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PETIT FROM BUILD..</td>\n",
       "      <td>[4, 21, 835, 2300]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BURGLARY CONVEYANCE</td>\n",
       "      <td>[23, 2300]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFORMATION</td>\n",
       "      <td>[332]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>PETIT OF BICYCLE</td>\n",
       "      <td>[4, 10, 436]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>PETIT OF VEHICLE..</td>\n",
       "      <td>[4, 10, 7, 2300]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>INFORMATION</td>\n",
       "      <td>[332]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>ASSAULT 3</td>\n",
       "      <td>[15, 481]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>5B</td>\n",
       "      <td>[2300]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                description            word_vec                  features\n",
       "0         FRAUD - ALL OTHER  [110, 13, 186, 35]  [0, 0, 0, 0, 0, 1, 0, 0]\n",
       "1                        6G         [1661, 145]  [0, 0, 0, 0, 0, 0, 0, 0]\n",
       "2        PETIT FROM BUILD..  [4, 21, 835, 2300]  [0, 0, 0, 0, 0, 0, 0, 0]\n",
       "3       BURGLARY CONVEYANCE          [23, 2300]  [0, 0, 0, 0, 0, 0, 0, 0]\n",
       "4               INFORMATION               [332]  [0, 0, 0, 0, 0, 0, 0, 0]\n",
       "...                     ...                 ...                       ...\n",
       "499995     PETIT OF BICYCLE        [4, 10, 436]  [0, 0, 0, 0, 0, 0, 0, 0]\n",
       "499996   PETIT OF VEHICLE..    [4, 10, 7, 2300]  [1, 0, 0, 0, 0, 0, 0, 0]\n",
       "499997          INFORMATION               [332]  [0, 0, 0, 0, 0, 0, 0, 0]\n",
       "499998            ASSAULT 3           [15, 481]  [0, 1, 0, 0, 0, 0, 0, 0]\n",
       "499999                   5B              [2300]  [0, 0, 0, 0, 0, 0, 0, 0]\n",
       "\n",
       "[500000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['word_vec'] = test_df['description'].astype(object)\n",
    "test_df['features'] = test_df['description'].astype(object)\n",
    "for idx, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
    "    sentence = row['description']\n",
    "    keywords_matched = {'Property Crime': 0, \n",
    "                    'Violent Crime': 0,\n",
    "                    'Substance-related Crime': 0, \n",
    "                    'Sex Crime': 0, \n",
    "                    'Personal Crime (nonviolent)': 0, \n",
    "                    'Financial Crime': 0,\n",
    "                    'No Crime Committed': 0, \n",
    "                    'Other': 0}\n",
    "    sentence = nlp(sentence)\n",
    "    new_sentence = []\n",
    "    for n, word in enumerate(sentence):\n",
    "        word_lower = word.lemma_.lower()\n",
    "        if word_lower in word_dict:\n",
    "            new_sentence.append(word_dict[word_lower])\n",
    "        else:\n",
    "            new_sentence.append(word_dict['<UNK>'])\n",
    "        if word_lower in inv_keyword_dict:\n",
    "            keywords_matched[inv_keyword_dict[word_lower]] += 1\n",
    "    test_df.at[idx, 'word_vec'] = new_sentence\n",
    "    test_df.at[idx, 'features'] = list(keywords_matched.values())\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 44]) torch.Size([8, 8]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CrimeDiscriptions(Dataset):\n",
    "    def __init__(self, df, word_dict, test=False):\n",
    "        self.data = df['word_vec']\n",
    "        self.features = df['features']\n",
    "        sent_len = self.data.apply(len)\n",
    "        self.pad_length = sent_len.max()\n",
    "        self.word_dict = word_dict\n",
    "        self.test = test\n",
    "        if not test:\n",
    "            self.lables = df['category']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data_pad = self.data.iloc[idx]\n",
    "        data_pad += [self.word_dict['<PAD>']] * (self.pad_length - len(data_pad))\n",
    "        out = [torch.as_tensor(data_pad).float(), torch.as_tensor(self.features.iloc[idx]).float()]\n",
    "        if not self.test:\n",
    "            out.append(torch.as_tensor(self.lables.iloc[idx]).float())\n",
    "        return out\n",
    "\n",
    "transformed_dataset = CrimeDiscriptions(train_df, word_dict)\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=8, shuffle=True)\n",
    "for data, features, labels in dataloader:\n",
    "    print(data.shape, features.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the data transformed and ready, we can move to our model.\n",
    "\n",
    "<br><br>\n",
    "<h3>3) Model</h3>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "epochs = 25\n",
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "dropout_prob = 0.5\n",
    "train_tvalid_split_ratio = 0.8\n",
    "num_labels = 8\n",
    "embedding_dim = 64\n",
    "hidden_size = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, dropout_prob, hidden_size=8, num_labels=8):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, dropout=dropout_prob, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size + num_labels, num_labels)\n",
    "    def forward(self, x, features):\n",
    "        x = self.embeddings(x)\n",
    "        out, (ht, ct) = self.lstm(x)\n",
    "        out = torch.cat((ht[-1], features), 1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(len(word_dict), embedding_dim, dropout_prob, hidden_size, num_labels).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = CrimeDiscriptions(train_df, word_dict)\n",
    "train_size = int(len(transformed_dataset) * train_tvalid_split_ratio)\n",
    "test_size = len(transformed_dataset) - train_size\n",
    "train_set, valid_set = torch.utils.data.random_split(transformed_dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functin to remove predictions with low confidence \n",
    "def remove_low_conf(y_true, y_pred, threshold=0.65):\n",
    "    y_pred = nn.functional.softmax(y_pred)\n",
    "    y_pred, y_pred_class = y_pred.max(1)\n",
    "    low_conf = y_pred < threshold\n",
    "    y_pred_class[low_conf] = -1\n",
    "    y_true = y_true.clone()\n",
    "    y_true[low_conf] = -1\n",
    "    return y_true, y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-166be8afa6b6>:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred = nn.functional.softmax(y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain loss: 345.417\tTrain Acc: 0.999\tTrain unclassified: 0.012\tValid loss: 14.455\tValid Acc: 1.000\tValid unclassified: 0.004\n",
      "Epoch: 1\tTrain loss: 46.710\tTrain Acc: 0.999\tTrain unclassified: 0.002\tValid loss: 10.844\tValid Acc: 0.999\tValid unclassified: 0.000\n",
      "Epoch: 2\tTrain loss: 35.904\tTrain Acc: 1.000\tTrain unclassified: 0.002\tValid loss: 11.021\tValid Acc: 1.000\tValid unclassified: 0.003\n",
      "Epoch: 3\tTrain loss: 33.208\tTrain Acc: 1.000\tTrain unclassified: 0.002\tValid loss: 10.373\tValid Acc: 1.000\tValid unclassified: 0.003\n",
      "Epoch: 4\tTrain loss: 31.448\tTrain Acc: 1.000\tTrain unclassified: 0.002\tValid loss: 10.476\tValid Acc: 1.000\tValid unclassified: 0.003\n",
      "Epoch: 5\tTrain loss: 30.733\tTrain Acc: 1.000\tTrain unclassified: 0.002\tValid loss: 9.930\tValid Acc: 1.000\tValid unclassified: 0.003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-21e40169e8ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/HiGeorge/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/HiGeorge/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    train_correct, train_total, train_low_conf = 0, 0, 0\n",
    "    for data, features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.long().to(device), features.to(device))\n",
    "        loss = criterion(out.cpu(), labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        labels, out = remove_low_conf(labels, out)\n",
    "        low_conf = (labels == -1).sum()\n",
    "        train_correct += (labels == out).sum() - low_conf\n",
    "        train_total += labels.shape[0] - low_conf\n",
    "        train_low_conf += low_conf\n",
    "    valid_loss = 0\n",
    "    valid_correct, valid_total, valid_low_conf = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, features, labels in valid_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.long().to(device), features.to(device))\n",
    "            loss = criterion(out.cpu(), labels.long())\n",
    "            valid_loss += loss.item()\n",
    "            labels, out = remove_low_conf(labels, out)\n",
    "            low_conf = (labels == -1).sum()\n",
    "            valid_correct += (labels == out).sum() - low_conf\n",
    "            valid_total += labels.shape[0] - low_conf\n",
    "            valid_low_conf += low_conf\n",
    "    train_acc = train_correct/(train_total+1e-6)\n",
    "    valid_acc = valid_correct/(valid_total+1e-6)\n",
    "    train_low_conf = train_low_conf/(train_total+train_low_conf)\n",
    "    valid_low_conf = valid_low_conf/(valid_total+valid_low_conf)\n",
    "    print(\"Epoch: {}\\tTrain loss: {:.3f}\\tTrain Acc: {:.3f}\\tTrain unclassified: {:.3f}\\tValid loss: {:.3f}\\tValid Acc: {:.3f}\\tValid unclassified: {:.3f}\".format(\n",
    "        epoch, epoch_loss, train_acc, train_low_conf, valid_loss, valid_acc, valid_low_conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model trained, lets evaluate some metrics on our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-166be8afa6b6>:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred = nn.functional.softmax(y_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:                precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       1.00      1.00      1.00       344\n",
      "         0.0       1.00      1.00      1.00     77808\n",
      "         1.0       1.00      1.00      1.00     23730\n",
      "         2.0       1.00      1.00      1.00      1278\n",
      "         3.0       1.00      1.00      1.00      1082\n",
      "         4.0       0.99      0.99      0.99       317\n",
      "         5.0       1.00      0.99      1.00       769\n",
      "         6.0       1.00      1.00      1.00      1075\n",
      "         7.0       1.00      1.00      1.00      7105\n",
      "\n",
      "    accuracy                           1.00    113508\n",
      "   macro avg       1.00      1.00      1.00    113508\n",
      "weighted avg       1.00      1.00      1.00    113508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred, y_true = [], []\n",
    "valid_correct, valid_total, valid_low_conf = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for data, features, labels in valid_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.long().to(device), features.to(device))\n",
    "        labels, out = remove_low_conf(labels, out)\n",
    "        y_pred.append(out)\n",
    "        y_true.append(labels)\n",
    "        low_conf = (labels == -1).sum()\n",
    "        valid_correct += (labels == out).sum() - low_conf\n",
    "        valid_total += labels.shape[0] - low_conf\n",
    "        valid_low_conf += low_conf\n",
    "print('Precision: ', classification_report(torch.cat(y_true), torch.cat(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions with lower confidence were classified as class -1. Out of the 20000 examples in validation set, 7 were classfied as -1. The precision scores for all the classes were high, and if even higher precision is needed, the remove_low_conf takes an additional threshold parameter which can be set to a higher value. \n",
    "\n",
    "<br><br>\n",
    "The only task left is to classify the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 30380/500000 [01:09<17:51, 438.47it/s]"
     ]
    }
   ],
   "source": [
    "transformed_dataset_test = CrimeDiscriptions(test_df, word_dict, test=True)\n",
    "test_loader = DataLoader(transformed_dataset_test, batch_size=1, shuffle=True)\n",
    "\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for data, features in tqdm(test_loader):\n",
    "        out = model(data.long().to(device), features.to(device))\n",
    "        out = out.max(1)[1]\n",
    "        y_pred.append(out)\n",
    "y_pred = torch.cat(y_pred)\n",
    "test_df['prediction'] = y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['prediction'] = test_df['prediction'].apply(lambda x: inc_encoding_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(os.path.join(data_path, 'CrimeDescriptions.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary: \n",
    "<br>\n",
    "- CrimeDescriptions.csv file has the classes filled in with the predictions.<br>\n",
    "- The precision scores for all the classes are high, but we have another class with for items classified with low confidence. <br>\n",
    "- Additional feature extracted in form word word matching helped increasing the precision scores. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
